from pyspark.sql import SparkSession
from pyspark.sql.functions import year, expr

# Create SparkSession
spark = SparkSession.builder \
    .appName("Yearly Trend Analysis - Median") \
    .getOrCreate()

# Assuming trust_df is already created with the data

# Extract year from Period column
trust_df = trust_df.withColumn("Year", year("Period"))

# Calculate median Type1_Departments_Major_A&E value for each year
median_expr = expr("approxQuantile(Type1_Departments_Major_A&E, 0.5, 10000)[0]")
median_analysis_df = trust_df.groupBy("Year") \
    .agg(median_expr.alias("Median_Type1_Departments_Major_A&E"))

# Calculate the overall median
overall_median = trust_df.approxQuantile("Type1_Departments_Major_A&E", [0.5], 10000)[0]

# Calculate the average of medians for each year
trend_analysis_df = median_analysis_df \
    .selectExpr("Year", f"'{overall_median}' AS Overall_Median", "Median_Type1_Departments_Major_A&E") \
    .withColumn("Yearly_Trend", expr("Median_Type1_Departments_Major_A&E - Overall_Median"))

# Show the trend analysis
trend_analysis_df.orderBy("Year").show()

# Stop SparkSession
spark.stop()


import pandas as pd
import matplotlib.pyplot as plt

# Convert Spark DataFrame to Pandas DataFrame
trend_analysis_pd = trend_analysis_df.toPandas()

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(trend_analysis_pd['Year'], trend_analysis_pd['Avg_Type1_Departments_Major_A&E'], marker='o', linestyle='-')
plt.title('Yearly Trend Analysis of Type1_Departments_Major_A&E')
plt.xlabel('Year')
plt.ylabel('Average Type1_Departments_Major_A&E')
plt.grid(True)
plt.xticks(trend_analysis_pd['Year'])
plt.tight_layout()
plt.show()
